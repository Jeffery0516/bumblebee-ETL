etlAgent.sources = src1
etlAgent.sinks = sink1

# Describe/configure the source
etlAgent.sources.src1.type = com.gsta.bigdata.etl.flume.kafka.KafkaSource
etlAgent.sources.src1.kafka.topics=flumetest
etlAgent.sources.src1.kafka.consumer.group.id=gdnoce1
etlAgent.sources.src1.kafka.bootstrap.servers=10.17.35.109:9092
etlAgent.sources.src1.maxBatchSize=2000
etlAgent.sources.src1.interceptors = gzDPIInterceptor
etlAgent.sources.src1.interceptors.gzDPIInterceptor.type = com.gsta.bigdata.etl.flume.GZDPIInterceptor$Builder
etlAgent.sources.src1.interceptors.gzDPIInterceptor.fields=ts,ad,srcip,srcport,dstip,dstport,url,host,path,query,domain,ref,ua,cookie,weixinid,qq,buy_uin,taobao_nick,weibosup,weiboname,weibonick,hour,timeStamp,day,collectHost
etlAgent.sources.src1.interceptors.gzDPIInterceptor.delimiter="\|"
etlAgent.sources.src1.interceptors.gzDPIInterceptor.headerFields=day,hour

# Describe the sink
etlAgent.sinks.sink1.type = hdfs
etlAgent.sinks.sink1.hdfs.path=hdfs://10.17.35.119:8020/user/tianxq/flume/%{day}/%{hour}/
etlAgent.sinks.sink1.hdfs.fileSuffix=.txt
#2G file size
etlAgent.sinks.sink1.hdfs.rollSize=2147483648
#or 1800 seconds
etlAgent.sinks.sink1.hdfs.rollInterval=1800
#don't use record count
etlAgent.sinks.sink1.hdfs.rollCount=0
etlAgent.sinks.sink1.hdfs.batchSize=2000
etlAgent.sinks.sink1.hdfs.fileType=CompressedStream  
etlAgent.sinks.sink1.hdfs.codeC=lzo
etlAgent.sinks.sink1.hdfs.writeFormat=Text

# Use a channel which buffers events in memory
etlAgent.channels = ch1
etlAgent.channels.ch1.type = memory
etlAgent.channels.ch1.capacity = 50000
etlAgent.channels.ch1.transactionCapacity = 50000

# Bind the source and sink to the channel
etlAgent.sources.src1.channels = ch1
etlAgent.sinks.sink1.channel = ch1