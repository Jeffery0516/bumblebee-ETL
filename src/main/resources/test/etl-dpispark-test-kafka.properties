hosts=192.168.9.17,192.168.9.16,192.168.9.15
port=2181
#hosts=10.17.35.16
#port=12181
brokers=/kafka80/brokers
topic=dpi-60-3r-0120
group=etl-consumer0407-77
consumerZK=192.168.9.17:2181,192.168.9.16:2181,192.168.9.15:2181
#consumerZK=10.17.35.16:12181
consumerZKPath=/spark-kafka-test
#every batch size=fetchsizebytes*(duration/fillfreqms)*receivesNum
#duration unit is second
duration=10
receivesNum=60
fetchsizebytes=1048576
#fetchsizebytes=2097152
fillfreqms=1000
backpressure=true
proportional=1.1
#proportional=0.75
integral=1.1
#integral=0.15
derivative=0
forcefromstart=false
chechPoint=true
chkPointPath=hdfs\://192.168.9.16\:8020/data/spark/checkpoint
#0:MEMORY_ONLY 1:MEMORY_ONLY_2 2:MEMORY_ONLY_SER 
#3:MEMORY_ONLY_SER_2 4:MEMORY_AND_DISK 
#5:MEMORY_AND_DISK_2 6:MEMORY_AND_DISK_SER 7:MEMORY_AND_DISK_SER_2
storageLevel=2
#resultMode:hdfs or kafka
resultMode=kafka
outputKafkaBroker=192.168.9.80:9092,192.168.9.81:9092,192.168.9.80:9094,192.168.9.80:9093,192.168.9.81:9093,192.168.9.81:9094
#outputKafkaBroker=10.17.35.16:9092
outputTopic=dpi-result
#hdfs partition
partitionsNum=24
#produce env
#outputPath=hdfs\://10.17.35.17\:8020/data/spark/spark
outputPath=hdfs\://192.168.9.17\:8020/data/spark/spark
#test env
#outputPath=hdfs\://10.17.35.120\:8020/user/tianxq/spark/spark