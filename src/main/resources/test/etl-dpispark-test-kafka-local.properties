hosts=10.17.35.16
port=12181
brokers=/kafka80/brokers
topic=dpi-60-3r-0120
group=etl-consumer0407-77
consumerZK=10.17.35.16:12181
consumerZKPath=/spark-kafka-test
#every batch size=fetchsizebytes*(duration/fillfreqms)*receivesNum
#duration unit is second
duration=10
receivesNum=60
fetchsizebytes=1048576
#fetchsizebytes=2097152
fillfreqms=1000
backpressure=true
proportional=1.1
#proportional=0.75
integral=1.1
#integral=0.15
derivative=0
forcefromstart=false
chechPoint=false
chkPointPath=hdfs\://192.168.9.16\:8020/data/spark/checkpoint
#0:MEMORY_ONLY 1:MEMORY_ONLY_2 2:MEMORY_ONLY_SER 
#3:MEMORY_ONLY_SER_2 4:MEMORY_AND_DISK 
#5:MEMORY_AND_DISK_2 6:MEMORY_AND_DISK_SER 7:MEMORY_AND_DISK_SER_2
storageLevel=2
#resultMode:hdfs or kafka
resultMode=kafka
outputKafkaBroker=10.17.35.16:9092
outputTopic=dpi-result
#hdfs partition
partitionsNum=24
urlSuffixFile=D:/github/bumblebee-ETL/src/main/resources/test/suffix-urlfilter.txt
#produce env
#outputPath=hdfs\://10.17.35.17\:8020/data/spark/spark
outputPath=hdfs\://192.168.9.17\:8020/data/spark/spark
#test env
#outputPath=hdfs\://10.17.35.120\:8020/user/tianxq/spark/spark